{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f948fd9-8802-4ebf-8684-5f4c7f09c56e",
   "metadata": {},
   "source": [
    "# **Association between Abstract Characteristics and Ranking Position in Technology-Assisted Reviewing (TAR): data preprocessing**\n",
    "\n",
    "### **by Isa Spiero <br>**\n",
    "\n",
    "#### **Part I: Loading TRIPOD characteristics**\n",
    "In the first part, the datasets are loaded which contain the titles and abstracts of all records of the reviews including the corresponding labels for the the title-abstract level inclusions and full-text level inclusions. These datasets are merged with the datasets containing the TRIPOD characteristics of the full-text level inclusions of the reviews.\n",
    "\n",
    "#### **Part II: Adding structure characteristics**\n",
    "In the second part, the structural characteristics of the abstracts are derived and added to the datasets: the number of words in the abstract, the average sentence length, and the abstract structuring (structured vs unstructured). \n",
    "\n",
    "#### **Part III: Adding terminology characteristics**\n",
    "In the third part, the terminology usage of the abstract is computed by comparing the mean TF-IDF vectors with the average mean vector of the entire dataset as a measure of abberrant terminology usage per abstract, with larger values indicating more abberant abstracts than smaller values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ae2b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3bc5395",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90353eec-7dff-4e93-9501-c095ac491251",
   "metadata": {},
   "source": [
    "#### **Part I: Loading TRIPOD characteristics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62c5c88",
   "metadata": {},
   "source": [
    "**Part I.a Dataset based on the systematic review by Andaur Navarro *et al.* (2022): 'Completeness of reporting of clinical prediction models developed using supervised machine learning: a systematic review'**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d85b4a4",
   "metadata": {},
   "source": [
    "Load the data containing the TRIPOD scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d2f6139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n"
     ]
    }
   ],
   "source": [
    "df1_scores = pd.read_csv(path_data + 'raw/AndaurNavarro_et_al_2022/20201127_DATA_TRIPOD.csv')\n",
    "\n",
    "# Select only the relevant columns:\n",
    "df1_scores_sel = df1_scores[['article_id', \n",
    "                             't_title_1', 't_title_2', 't_title_3', 't_title_4',\n",
    "                             't_abstract_1', 't_abstract_2', 't_abstract_3', 't_abstract_4',\n",
    "                             't_abstract_5', 't_abstract_6', 't_abstract_7', 't_abstract_8',\n",
    "                             't_abstract_9', 't_abstract_10', 't_abstract_11', 't_abstract_12']]\n",
    "\n",
    "# Rename the columns:\n",
    "df1_scores_sel.rename(columns={'t_title_1': '1i',\n",
    "                               't_title_2': '1ii',\n",
    "                               't_title_3': '1iii',\n",
    "                               't_title_4': '1iv',\n",
    "                               't_abstract_1': '2i',\n",
    "                               't_abstract_2': '2ii',\n",
    "                               't_abstract_3': '2iii',\n",
    "                               't_abstract_4': '2iv',\n",
    "                               't_abstract_5': '2v',\n",
    "                               't_abstract_6': '2vi',\n",
    "                               't_abstract_7': '2vii',\n",
    "                               't_abstract_8': '2viii',\n",
    "                               't_abstract_9': '2ix',\n",
    "                               't_abstract_10': '2x',\n",
    "                               't_abstract_11': '2xi',\n",
    "                               't_abstract_12': '2xii',}, inplace=True)\n",
    "\n",
    "# Check that there are 152 inclusions in the review that were scored with TRIPOD\n",
    "print(len(df1_scores_sel))\n",
    "\n",
    "#df1_scores_sel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d29efc0f-17e9-4964-be3e-3830bc71d88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To check the unique values per column:\n",
    "\n",
    "# selected_columns = df1_scores_sel.filter(regex='^(1|2)')\n",
    "# # Get unique values for each selected column\n",
    "# unique_values_dict = {col:df1_scores_sel[col].unique() for col in selected_columns}\n",
    "\n",
    "# # Print unique values\n",
    "# for col, unique_values in unique_values_dict.items():\n",
    "#     print(f\"{col}: {unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "886457cd-ea24-40c4-83de-154fc629957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace values in column '2vi' for consistency:\n",
    "# Reason: based on file '20201021_CODEBOOK_TRIPOD.r'from the review derived from dataverse,\n",
    "# there were three levels, but only for criterion 2vi, and these were coded with 'YES', 'NO', and 'NA', respectively\n",
    "# All others were coded with 1='YES' and 0='NO'.\n",
    "df1_scores_sel['2vi'] = df1_scores_sel['2vi'].replace({1: 1, 2: 0, 3: np.nan})\n",
    "df1_scores_sel['2vi'] = df1_scores_sel['2vi'].astype('Int64')\n",
    "\n",
    "#df1_scores_sel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "729beb78-6f6d-407d-ab88-9a3083e34083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute total number of applicable items (= non-NaN values) in the TRIPOD scoring, the total number of reported (= 1) TRIPOD items, \n",
    "# and the percentage of reported of the applicable items:\n",
    "selected_cols = df1_scores_sel.filter(regex='^(1|2)')\n",
    "df1_scores_sel['total_applicable'] = selected_cols.notna().sum(axis=1)\n",
    "df1_scores_sel['total_reported'] = selected_cols.eq(1).sum(axis=1)\n",
    "df1_scores_sel['percentage_reported'] = df1_scores_sel['total_reported'] / df1_scores_sel['total_applicable'] * 100\n",
    "\n",
    "#df1_scores_sel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afca31f",
   "metadata": {},
   "source": [
    "Load the data containing the inclusion labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a17442c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n",
      "152\n",
      "312\n"
     ]
    }
   ],
   "source": [
    "# This file contains the abstracts, titles, and title-abstract level inclusions\n",
    "# The article_ids column was manually added based on JCMachineLearningSys-Datalinked_DATA_2024-10-02_1410.csv\n",
    "df1_labels = pd.read_excel(path_data + 'raw/AndaurNavarro_et_al_2022/Prog_reporting_labeled_ids.xlsx')\n",
    "\n",
    "# Only the 152 inclusions have an article_id (added manually to the file), fill the others with NA\n",
    "df1_labels['article_id'] = df1_labels['article_id'].fillna(0)\n",
    "df1_labels['article_id'] = df1_labels['article_id'].astype(int)\n",
    "print(df1_labels['article_id'].ne(0).sum())\n",
    "\n",
    "# Add the label '1' for each of 152 full-text level inclusions and leave it as '0' for the exclusions\n",
    "df1_labels['label_ft_included'] = np.where(df1_labels['article_id'] == 0, 0, 1)\n",
    "print(df1_labels['label_ft_included'].sum())\n",
    "\n",
    "# Check that the label '1' for each of the title-abstract level inclusions corresponds to the correct number of 312\n",
    "df1_labels.rename(columns={'label_included': 'label_ta_included'}, inplace=True)\n",
    "print(df1_labels['label_ta_included'].sum())\n",
    "\n",
    "#df1_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e116287-cc8b-4124-98dc-edd19e3e58b6",
   "metadata": {},
   "source": [
    "Merge the data containing the TRIPOD scores with the data containing the inclusion labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cbe1979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2482\n",
      "312\n",
      "152\n",
      "2330\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.merge(df1_labels, df1_scores_sel, on='article_id', how='outer')\n",
    "\n",
    "# Check that dataframe consists 2482 records in total:\n",
    "print(len(df1))\n",
    "# Check the number of title-abstract inclusions of 312:\n",
    "print(df1['label_ta_included'].sum())\n",
    "# Check the number of full-text inclusions of 152:\n",
    "print(df1['label_ft_included'].sum())\n",
    "# Check the number of NaN for the TRIPOD corresponds to the number of full-text exclusions of 2330:\n",
    "print(df1['1i'].isna().sum())\n",
    "\n",
    "#df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0c4431",
   "metadata": {},
   "source": [
    "**Part I.b Dataset based on the systematic review by Heus *et al.* (2018): 'Poor reporting of multivariable prediction model studies: towards a targeted implementation strategy of the TRIPOD statement'**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ec5313-55ba-4988-acb5-7d4b28423bcc",
   "metadata": {},
   "source": [
    "Load the data containing the TRIPOD scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23b70146-498a-4953-8b55-2dd70f32f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_scores = pd.read_excel(path_data + 'raw/Heus_et_al_2018/170509_Data_set_for_SPSS.xlsx', \n",
    "                           sheet_name='Overview (n=147)') \n",
    "\n",
    "# Select only the relevant columns:\n",
    "df2_scores_sel = df2_scores[\n",
    "    ['ID', 'Endnote ID', \n",
    "     '1i', '1ii', '1iii', '1iv',\n",
    "     '2i', '2ii', '2iii', '2iv', '2v',\n",
    "     '2vi', '2vii', '2viii', '2ix', '2x',\n",
    "     '2xi', '2xii', '2xiii', '2xiv', '2xv',\n",
    "     '1i.1', '1ii.1', '1iii.1', '1iv.1',\n",
    "     '2i.1', '2ii.1', '2iii.1', '2iv.1', '2v.1',\n",
    "     '2vi.1', '2vii.1', '2viii.1', '2ix.1', '2x.1',\n",
    "     '2xi.1', '2xii.1', '2xiii.1', '2xiv.1', '2xv.1',\n",
    "     '1i.2', '1ii.2', '1iii.2', '1iv.2',\n",
    "     '2i.2', '2ii.2', '2iii.2', '2iv.2', '2v.2',\n",
    "     '2vi.2', '2vii.2', '2viii.2', '2ix.2', '2x.2',\n",
    "     '2xi.2', '2xii.2', '2xiii.2', '2xiv.2', '2xv.2'\n",
    "    ]\n",
    "]\n",
    "\n",
    "# List of sets of columns to merge, since columns are spread according to prediction model type:\n",
    "columns_to_merge = [\n",
    "    ['1i', '1i.1', '1i.2'],\n",
    "    ['1ii', '1ii.1', '1ii.2'],\n",
    "    ['1iii', '1iii.1', '1iii.2'],\n",
    "    ['1iv', '1iv.1', '1iv.2'],\n",
    "    ['2i', '2i.1', '2i.2'],\n",
    "    ['2ii', '2ii.1', '2ii.2'],\n",
    "    ['2iii', '2iii.1', '2iii.2'],\n",
    "    ['2iv', '2iv.1', '2iv.2'],\n",
    "    ['2v', '2v.1', '2v.2'],\n",
    "    ['2vi', '2vi.1', '2vi.2'],\n",
    "    ['2vii', '2vii.1', '2vii.2'],\n",
    "    ['2viii', '2viii.1', '2viii.2'],\n",
    "    ['2ix', '2ix.1', '2ix.2'],\n",
    "    ['2x', '2x.1', '2x.2'],\n",
    "    ['2xi', '2xi.1', '2xi.2'],\n",
    "    ['2xii', '2xii.1', '2xii.2'],\n",
    "    ['2xiii', '2xiii.1', '2xiii.2'],\n",
    "    ['2xiv', '2xiv.1', '2xiv.2'],\n",
    "    ['2xv', '2xv.1', '2xv.2']\n",
    "]\n",
    "\n",
    "df2_scores_merg = df2_scores_sel.copy()  \n",
    "\n",
    "# Loop through each set of columns and merge them\n",
    "for cols in columns_to_merge:\n",
    "    df2_scores_merg[cols[0]] = df2_scores_merg[cols].apply(\n",
    "        lambda row: 1 if set(row.dropna().unique()) == {1} \n",
    "        # If all scores for item x are 0, set final score to 0\n",
    "        else 0 if set(row.dropna().unique()) == {0} \n",
    "        # If scores for item x are 0 or 1, set final score to 1\n",
    "        # regardless of how many 0s or 1s\n",
    "        else 1 if set(row.dropna().unique()) == {0, 1} \n",
    "        # If score for item x is only 3 (which codes for NA, set final score to NA\n",
    "        else float('nan') if set(row.dropna().unique()).issubset({3}) \n",
    "        else float('nan'), \n",
    "        axis=1\n",
    "    )\n",
    "    # Drop the merged columns except the first one\n",
    "    df2_scores_merg = df2_scores_merg.drop(columns=cols[1:])\n",
    "\n",
    "#df2_scores_merg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fd8f5ad-1e27-48a7-8dda-50e8fd4df3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To check the unique values per column:\n",
    "\n",
    "# selected_columns = df2_scores_merg.filter(regex='^(1|2)')\n",
    "# # Get unique values for each selected column\n",
    "# unique_values_dict = {col:df2_scores_merg[col].unique() for col in selected_columns}\n",
    "\n",
    "# # Print unique values\n",
    "# for col, unique_values in unique_values_dict.items():\n",
    "#     print(f\"{col}: {unique_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e0e3bb-8bbb-48bd-9317-28e9d8d52708",
   "metadata": {},
   "source": [
    "Load the data containing the inclusion labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d76ad14-33c7-4802-8ffc-0710f1d924e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4871\n",
      "347\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>authors</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>journal</th>\n",
       "      <th>pmid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>abstract</th>\n",
       "      <th>language</th>\n",
       "      <th>label_ta_included</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "      <td>Factors associated with short-term changes in ...</td>\n",
       "      <td>AIDS</td>\n",
       "      <td>24959963</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OBJECTIVES: Among antiretroviral therapy (ART)...</td>\n",
       "      <td>eng</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "      <td>Reducing Injury Risk From Body Checking in Boy...</td>\n",
       "      <td>Pediatrics</td>\n",
       "      <td>24864185</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ice hockey is an increasingly popular sport th...</td>\n",
       "      <td>Eng</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "      <td>Hypothermia and Neonatal Encephalopathy</td>\n",
       "      <td>Pediatrics</td>\n",
       "      <td>24864176</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Data from large randomized clinical trials ind...</td>\n",
       "      <td>Eng</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "      <td>Incontinence: Leak point pressure predicts suc...</td>\n",
       "      <td>Nat Rev Urol</td>\n",
       "      <td>24861330</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eng</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "      <td>Identifying Gene-Environment Interactions in S...</td>\n",
       "      <td>Schizophr Bull</td>\n",
       "      <td>24860087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Recent years have seen considerable progress i...</td>\n",
       "      <td>eng</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             type authors  year  \\\n",
       "0           0  Journal Article     NaN  2014   \n",
       "1           1  Journal Article     NaN  2014   \n",
       "2           2  Journal Article     NaN  2014   \n",
       "3           3  Journal Article     NaN  2014   \n",
       "4           4  Journal Article     NaN  2014   \n",
       "\n",
       "                                               title         journal  \\\n",
       "0  Factors associated with short-term changes in ...            AIDS   \n",
       "1  Reducing Injury Risk From Body Checking in Boy...      Pediatrics   \n",
       "2            Hypothermia and Neonatal Encephalopathy      Pediatrics   \n",
       "3  Incontinence: Leak point pressure predicts suc...    Nat Rev Urol   \n",
       "4  Identifying Gene-Environment Interactions in S...  Schizophr Bull   \n",
       "\n",
       "       pmid keywords                                           abstract  \\\n",
       "0  24959963      NaN  OBJECTIVES: Among antiretroviral therapy (ART)...   \n",
       "1  24864185      NaN  Ice hockey is an increasingly popular sport th...   \n",
       "2  24864176      NaN  Data from large randomized clinical trials ind...   \n",
       "3  24861330      NaN                                                NaN   \n",
       "4  24860087      NaN  Recent years have seen considerable progress i...   \n",
       "\n",
       "  language  label_ta_included  \n",
       "0      eng                  0  \n",
       "1      Eng                  0  \n",
       "2      Eng                  0  \n",
       "3      eng                  0  \n",
       "4      eng                  0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_labels = pd.read_excel(path_data + 'raw/Heus_et_al_2018/Prog_tripod_labeled.xlsx')\n",
    "\n",
    "# Check the total number of records equals 4871:\n",
    "print(len(df2_labels))\n",
    "\n",
    "# Check that the number of title-abstract level inclusions equals 347:\n",
    "df2_labels.rename(columns={'label_included': 'label_ta_included'}, inplace=True)\n",
    "print(df2_labels['label_ta_included'].sum())\n",
    "\n",
    "df2_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15516976-97aa-4aee-8bb3-159a3dfe1d02",
   "metadata": {},
   "source": [
    "Load the data with the PubMed ID's (pmid) to link the TRIPOD scores with the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d12840a-7cbf-438b-abb0-97f1c03c6cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to parse the ris file\n",
    "def parse_ris_file(file_path):\n",
    "    references = []\n",
    "    entry = {}\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if line.strip() == \"\":  \n",
    "                continue\n",
    "\n",
    "            if line.startswith(\"TY  -\"):  \n",
    "                if entry:  \n",
    "                    references.append(entry)\n",
    "                entry = {}  \n",
    "            try:\n",
    "                tag, value = line.split('  - ', 1)  \n",
    "                entry[tag] = value.strip()  \n",
    "            except ValueError:\n",
    "                continue  \n",
    "\n",
    "        if entry:\n",
    "            references.append(entry)\n",
    "\n",
    "    return references\n",
    "\n",
    "# Convert parsed data into a pandas DataFrame\n",
    "parsed_data = parse_ris_file(path_data + 'raw/Heus_et_al_2018/TRIPOD adherence included_final-Converted.txt')\n",
    "df2_link = pd.DataFrame(parsed_data)\n",
    "\n",
    "# The column 'AN' contains the PubMed IDs:\n",
    "#df2_link.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e4322f-fda3-4795-aabf-f19beb8ac74c",
   "metadata": {},
   "source": [
    "Merge the PubMed IDs with the dataframe containing the TRIPOD scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1905dde-3259-4df7-92d3-7ee2ed2bb7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert both column types to string:\n",
    "df2_scores_merg['ID'] = df2_scores_merg['ID'].astype(str)\n",
    "df2_link['ID'] = df2_link['ID'].astype(str)\n",
    "\n",
    "# Merge the PubMed ID's with the dataframe with the TRIPOD scores:\n",
    "df2_scores_pmid = df2_scores_merg.merge(df2_link[['ID', 'AN']], on='ID', how='left')\n",
    "\n",
    "# Rename the column:\n",
    "df2_scores_pmid.rename(columns={'AN': 'pmid'}, inplace=True)\n",
    "\n",
    "# Add a column for full-text level inclusions:\n",
    "df2_scores_pmid['label_ft_included'] = int(1)\n",
    "\n",
    "# Compute total number of applicable items (= non-NaN values) in the TRIPOD scoring, the total number of reported (= 1) TRIPOD items, \n",
    "# and the percentage of reported of the applicable items:\n",
    "selected_cols = df2_scores_pmid.filter(regex='^(1|2)')\n",
    "df2_scores_pmid['total_applicable'] = selected_cols.notna().sum(axis=1)\n",
    "df2_scores_pmid['total_reported'] = selected_cols.eq(1).sum(axis=1) + selected_cols.eq(0.5).sum(axis=1) * 0.5\n",
    "df2_scores_pmid['percentage_reported'] = df2_scores_pmid['total_reported'] / df2_scores_pmid['total_applicable'] * 100\n",
    "\n",
    "#df2_scores_pmid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e36f618-7967-4644-b724-1c143900ef33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4871\n",
      "347\n",
      "147.0\n",
      "4724\n"
     ]
    }
   ],
   "source": [
    "df2_scores_pmid['pmid'] = df2_scores_pmid['pmid'].astype(str)\n",
    "df2_labels['pmid'] = df2_labels['pmid'].astype(str)\n",
    "\n",
    "\n",
    "df2 = pd.merge(df2_labels, df2_scores_pmid, on='pmid', how='outer', sort=False)\n",
    "df2 = df2.set_index('pmid').reindex(df2_labels['pmid']).reset_index()\n",
    "\n",
    "\n",
    "# Change all NaN to 0 for full text level inclusions (inclusions are indicated with 1 already)\n",
    "df2['label_ft_included'] = df2['label_ft_included'].fillna(int(0))\n",
    "\n",
    "# Check that dataframe consists 4871 records in total:\n",
    "print(len(df2))\n",
    "# Check the number of title-abstract inclusions of 312:\n",
    "print(df2['label_ta_included'].sum())\n",
    "# Check the number of full-text inclusions of 147:\n",
    "print(df2['label_ft_included'].sum())\n",
    "# Check the number of NaN for the TRIPOD corresponds to the number of full-text exclusions of 2330:\n",
    "print(df2['2vii'].isna().sum())\n",
    "\n",
    "#df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8079a407-9cc3-4bfa-8dd6-5dabed07c91d",
   "metadata": {},
   "source": [
    "#### **Part II: Adding structure characteristics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5023beeb-5f7d-41c0-8a37-ebddd0c5b167",
   "metadata": {},
   "source": [
    "Add the number of words per abstract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a286ae5-ebf5-4bbb-9b4e-424301db1774",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"word_count\"] = df1[\"abstract\"].apply(lambda x: len(str(x).split()))\n",
    "df2[\"word_count\"] = df2[\"abstract\"].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc3c491-b821-4470-b53d-bb246fb15262",
   "metadata": {},
   "source": [
    "Add the average sentence length per abstract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c56a4d82-2e3a-4d70-9976-30735318f393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sentence_length(text):\n",
    "    sentences = sent_tokenize(str(text)) \n",
    "    if len(sentences) == 0:\n",
    "        return 0  \n",
    "    word_counts = [len(sentence.split()) for sentence in sentences]  \n",
    "    return sum(word_counts) / len(sentences)  \n",
    "    \n",
    "df1[\"avg_sentence_length\"] = df2[\"abstract\"].apply(avg_sentence_length)\n",
    "df2[\"avg_sentence_length\"] = df2[\"abstract\"].apply(avg_sentence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e38c21-93f2-4a3a-895b-8a4d89dc5e2a",
   "metadata": {},
   "source": [
    "Add structured (1) vs unstructured (0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adc64db8-efcc-4c08-b8ff-508f28e146f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the distinction based on the most common words for structured abstracts to start with:\n",
    "keywords = [\"background\", \"objective\", \"objectives\", \"purpose\", \"introduction\", \"aim\", \"aims\"] \n",
    "\n",
    "df1[\"structured\"] = df1[\"abstract\"].apply(\n",
    "    lambda x: 1 if str(x).split(\":\")[0].lower() in [keyword.lower() for keyword in keywords] else 0)\n",
    "\n",
    "df2[\"structured\"] = df2[\"abstract\"].apply(\n",
    "    lambda x: 1 if str(x).split(\":\")[0].lower() in [keyword.lower() for keyword in keywords] else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534a983d-a2a1-41f6-b05d-487bf3f60670",
   "metadata": {},
   "source": [
    "#### **Part III: Adding terminology characteristics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbe9399-d871-4951-a9bc-8041c86dd91b",
   "metadata": {},
   "source": [
    "Compute the deviation in terminology usage using TF-IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d31af54d-db06-44ab-9e64-79cf008b8992",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Make NaN abstracts empty for vectorization\n",
    "df1[\"abstract\"] = df1[\"abstract\"].fillna(\"\")\n",
    "df2[\"abstract\"] = df2[\"abstract\"].fillna(\"\")\n",
    "\n",
    "# Filter only rows where 'label_ft_included' == 1\n",
    "df1_filtered = df1[df1[\"label_ft_included\"] == 1].copy()\n",
    "df2_filtered = df2[df2[\"label_ft_included\"] == 1].copy()\n",
    "\n",
    "# Create the vectors only for filtered rows\n",
    "tfidf_matrix1 = vectorizer.fit_transform(df1_filtered[\"abstract\"])\n",
    "tfidf_matrix2 = vectorizer.fit_transform(df2_filtered[\"abstract\"])\n",
    "\n",
    "# Compute the mean of each vector per abstract\n",
    "df1_filtered[\"tfidf_mean\"] = [np.nan if text == \"\" else score for text, score in zip(df1_filtered[\"abstract\"], tfidf_matrix1.mean(axis=1).A1)]\n",
    "df2_filtered[\"tfidf_mean\"] = [np.nan if text == \"\" else score for text, score in zip(df2_filtered[\"abstract\"], tfidf_matrix2.mean(axis=1).A1)]\n",
    "\n",
    "# Compute the average of the means of all vectors (only for included rows) across abstracts\n",
    "average_tfidf1 = df1_filtered[\"tfidf_mean\"].mean()\n",
    "average_tfidf2 = df2_filtered[\"tfidf_mean\"].mean()\n",
    "\n",
    "# Compute the deviation of the mean of an abstract from the overall average across all abstracts\n",
    "df1_filtered[\"tfidf_deviation\"] = df1_filtered[\"tfidf_mean\"].apply(lambda x: np.nan if pd.isna(x) else x - average_tfidf1)\n",
    "df2_filtered[\"tfidf_deviation\"] = df2_filtered[\"tfidf_mean\"].apply(lambda x: np.nan if pd.isna(x) else x - average_tfidf2)\n",
    "\n",
    "df1_filtered[\"tfidf_deviation\"] = df1_filtered[\"tfidf_deviation\"].abs()\n",
    "df2_filtered[\"tfidf_deviation\"] = df2_filtered[\"tfidf_deviation\"].abs()\n",
    "\n",
    "# Merge the results back into the original DataFrames\n",
    "df1 = df1.merge(df1_filtered[[\"tfidf_mean\", \"tfidf_deviation\"]], how=\"left\", left_index=True, right_index=True)\n",
    "df2 = df2.merge(df2_filtered[[\"tfidf_mean\", \"tfidf_deviation\"]], how=\"left\", left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76ed6894-f6dc-4a2e-8ee6-9240391a30ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing completed\n"
     ]
    }
   ],
   "source": [
    "# Export the processed files\n",
    "# Note: keep original names of 'Prog1' and 'Prog3' to be able to merge with rankings of previous simulations that had this naming\n",
    "df1.to_csv(path_data + 'processed/Prog1_reporting.csv', index=False)\n",
    "df2.to_csv(path_data + 'processed/Prog3_tripod.csv', index=False)\n",
    "\n",
    "df1.to_excel(path_data + 'processed/Prog1_reporting.xlsx', index=False)\n",
    "df2.to_excel(path_data + 'processed/Prog3_tripod.xlsx', index=False)\n",
    "\n",
    "print(\"Data preprocessing completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
