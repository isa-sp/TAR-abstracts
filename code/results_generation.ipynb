{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed9154e6-8f98-481b-973a-e6007662c986",
   "metadata": {},
   "source": [
    "# **Write Your Abstracts Carefully - The Impact of Abstract Reporting Quality on Findability by Semi-Automated Title-Abstract Screening Tools**\n",
    "### **Generation of results**\n",
    "\n",
    "#### **Part I: Data**\n",
    "\n",
    "In Part I, the datasets are imported. These are the datasets from the previously conducted systematic reviews by Andaur Navarro *et al.* (2022) and Heus *et al.* (2018), respectively. For each review, the titles and abstracts of the records along with the title-abstract level inclusions, full-text level inclusions, and abstract characteristics are present in the dataset. The abstract characterstics consist of: (I) abstract reporting quality as defined by TRIPOD scores, (II) abstract structural components, and (III) abstract language usage.\n",
    "\n",
    "#### **Part II: Ranking** \n",
    "\n",
    "A screening tool (ASReview) was used to simulate the title-abstract screening by using active learning to rank and prioritize records that are most likely to be relevant for the review. These ranking algorithms use a feature extractor and classifier to compute rankings. The simulations were continued until all records have passed, and the final rankings were saved and are here merged with the datasets.\n",
    "\n",
    "#### **Part III: Evaluation**\n",
    "\n",
    "In the evaluation, the association between abstract characteristics (as described under *Data*) and the ranking position (as computed with the tool under *Ranking*) is examined.\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e0880c0-d93e-4df1-a831-302471432a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from pathlib import Path\n",
    "from scipy.stats import spearmanr, mannwhitneyu\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627598d2-0479-4e03-89ba-2456436aec7d",
   "metadata": {},
   "source": [
    "#### **Part I: Data**\n",
    "Load the data that were preprocessed with the data_preprocessing.ipynb notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a74ea072-32bc-4550-ab4a-b6bbc0b04a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"../data/processed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89eaa234-8af0-43b4-938d-53af56f2a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dic = {}\n",
    "\n",
    "for file_name in os.listdir(path_data):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(path_data, file_name)\n",
    "        df = pd.read_csv(file_path)\n",
    "        key = os.path.splitext(file_name)[0].split(\"_\")[0]\n",
    "        review_dic[key] = df\n",
    "\n",
    "review_dic = dict(sorted(review_dic.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91e23d46-bc05-4ce5-885d-c2c569c574bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in review_dic.items():\n",
    "    df.rename(columns={'Unnamed: 0': 'record_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f394c8-fbd3-422e-9c96-b907041d75c1",
   "metadata": {},
   "source": [
    "#### **Part II: Ranking**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be003b0-f919-4926-825a-af5ee86d8f59",
   "metadata": {},
   "source": [
    "Load rankings of previous study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d10355c-6806-418e-ad2c-7ec89470441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_output = \"../output/rankings/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72e4760d-0df5-4478-aca2-7feac3fa61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_models = ['logistic'] \n",
    "feature_models = ['tfidf', 'sbert', 'doc2vec']\n",
    "query_models = ['max']\n",
    "n_simulations = 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06ebf9da-de6f-4af0-9a39-2b3a1e4bd794",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_list_names = []\n",
    "for review in review_dic:\n",
    "    for train_model in train_models:\n",
    "        for feature_model in feature_models:\n",
    "            for query_model in query_models:\n",
    "                review_id = str(review + \"_\" + train_model + \"_\" + feature_model + \"_\" + query_model )\n",
    "                sim_list_names.append(review_id)\n",
    "             \n",
    "multiple_sims = []\n",
    "for i in range(0, len(sim_list_names)):\n",
    "    raw_output = {}\n",
    "    for j in range(1,n_simulations+1):\n",
    "        if Path(path_output +'sim_{review_id}_{sim}.p'.format(review_id=sim_list_names[i], sim=j)).is_file():\n",
    "            with open(path_output + 'sim_{review_id}_{sim}.p'.format(review_id=sim_list_names[i], sim=j),'rb') as f:\n",
    "                raw_output.update(pickle.load(f))\n",
    "    if len(raw_output) > 0:\n",
    "        multiple_sims.append((sim_list_names[i], len(review_dic[sim_list_names[i].split('_')[0]]), n_simulations, raw_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ae79fc-d833-4728-9311-7f3c8f425aa5",
   "metadata": {},
   "source": [
    "Merge the rankings with the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c8a449f-7608-4888-84f3-f4b66bc4d13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "review_dic_merged = review_dic.copy()\n",
    "\n",
    "for i in review_dic:\n",
    "    for j in range(0, len(multiple_sims)):\n",
    "        if multiple_sims[j][0].split('_')[0] == i:\n",
    "\n",
    "            for k in multiple_sims[j][3]:          \n",
    "\n",
    "                review_sim = multiple_sims[j][0] + \"_\" + str(j)\n",
    "                review_key = multiple_sims[j][0].split('_')[0]\n",
    "                index_list = multiple_sims[j][3][k][0]['record_id']\n",
    "                review_dic_merged[review_key][review_sim] = review_dic_merged[review_key]['record_id'].map({val: idx for idx, val in enumerate(index_list)})\n",
    "            \n",
    "    review_dic_merged[review_key]['ranking_average_logistic_tfidf'] = review_dic_merged[review_key].filter(like='logistic_tfidf').mean(axis=1)\n",
    "    review_dic_merged[review_key]['ranking_average_logistic_sbert'] = review_dic_merged[review_key].filter(like='logistic_sbert').mean(axis=1)\n",
    "    review_dic_merged[review_key]['ranking_average_logistic_doc2vec'] = review_dic_merged[review_key].filter(like='logistic_doc2vec').mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aca4762-d5ef-4d91-947a-b612dff91ef7",
   "metadata": {},
   "source": [
    "#### **Part III: Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f5a57a-e7ba-4719-a00d-971070e1e4b5",
   "metadata": {},
   "source": [
    "Convert the data into a presentable table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6daaa726-456f-4e33-8ea8-61fb9fafd389",
   "metadata": {},
   "outputs": [],
   "source": [
    "characteristics = ['1i', '1ii', '1iii', '1iv',\n",
    "                   '2i', '2ii', '2iii', '2iv', '2v',\n",
    "                   '2vi', '2vii', '2viii', '2ix', '2x',\n",
    "                   '2xi', '2xii',\n",
    "                   'percentage_reported',\n",
    "                   'word_count', 'avg_sentence_length', 'structured',\n",
    "                   'tfidf_deviation'\n",
    "                  ]\n",
    "review = ['Prog1', 'Prog3'\n",
    "         ]\n",
    "rankings = ['ranking_average_logistic_tfidf', \n",
    "            'ranking_average_logistic_sbert',\n",
    "            'ranking_average_logistic_doc2vec'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb5b4651-246b-4422-9790-0667a4dfeee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for review_name in review:\n",
    "    df = review_dic_merged[review_name]  \n",
    "    \n",
    "\n",
    "    for characteristic in characteristics:\n",
    "        \n",
    "        unique_values = df[characteristic].dropna().unique()\n",
    "        is_binary = set(unique_values) == {0, 1}  \n",
    "\n",
    "        for ranking in rankings:\n",
    "\n",
    "            df_clean = df[[characteristic, ranking]].dropna()\n",
    "            \n",
    "            if characteristic == 'tfidf_deviation':\n",
    "                df_clean['standardized_tfidf_deviation'] = (df_clean[characteristic] - df_clean[characteristic].mean()) / df_clean[characteristic].std()\n",
    "            \n",
    "            if len(df_clean) > 1:  \n",
    "                result_dict = {\n",
    "                    'Review': review_name,\n",
    "                    'Characteristic': characteristic,\n",
    "                    'Ranking method': ranking,\n",
    "                    'Correlation type': 'Binary' if is_binary else 'Continuous',\n",
    "                    'Correlation coefficient': None,\n",
    "                    'p-value': None,\n",
    "                    'Beta': None,\n",
    "                    'R-squared': None,\n",
    "                    'Event rate/Mean (sd)': None\n",
    "                }\n",
    "                \n",
    "                if is_binary:\n",
    "                    \n",
    "                    event_rate = df_clean[characteristic].mean()\n",
    "\n",
    "                    group1 = df_clean[df_clean[characteristic] == 0][ranking]\n",
    "                    group2 = df_clean[df_clean[characteristic] == 1][ranking]\n",
    "                    \n",
    "                    if len(group1) > 0 and len(group2) > 0:\n",
    "                        U, p_value = mannwhitneyu(group1, group2, alternative='two-sided')\n",
    "                        n1, n2 = len(group1), len(group2)\n",
    "                        rank_biserial_corr = 1 - (2 * U / (n1 * n2))  \n",
    "\n",
    "                        result_dict['Correlation coefficient'] = rank_biserial_corr\n",
    "                        result_dict['p-value'] = p_value\n",
    "                    \n",
    "                    result_dict['Event rate/Mean (sd)'] = f\"{event_rate:.2f}\"\n",
    "                \n",
    "                else:\n",
    "    \n",
    "                    mean_val = df_clean[characteristic].mean()\n",
    "                    std_val = df_clean[characteristic].std()\n",
    "\n",
    "                    if characteristic == 'tfidf_deviation':\n",
    "                        X = sm.add_constant(df_clean['standardized_tfidf_deviation'])  \n",
    "                    else:\n",
    "                        X = sm.add_constant(df_clean[characteristic])\n",
    "                    \n",
    "                    model = sm.OLS(df_clean[ranking], X).fit()\n",
    "                    \n",
    "                    result_dict['Correlation coefficient'] = spearmanr(df_clean[characteristic], df_clean[ranking])[0]\n",
    "                    result_dict['p-value'] = spearmanr(df_clean[characteristic], df_clean[ranking])[1]\n",
    "                    result_dict['Beta'] = model.params[1] \n",
    "                    result_dict['R-squared'] = model.rsquared\n",
    "\n",
    "                    result_dict['Event rate/Mean (sd)'] = f\"{mean_val:.2f} ({std_val:.2f})\"\n",
    "                \n",
    "                results.append(result_dict)\n",
    "\n",
    "correlation_df = pd.DataFrame(results)\n",
    "correlation_df = correlation_df.applymap(lambda x: round(x, 3) if isinstance(x, (int, float)) else x)\n",
    "\n",
    "#correlation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b8ae7-b8cb-44ce-bea8-6103ffd046ab",
   "metadata": {},
   "source": [
    "Clean the dataframe for publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c9995a5-dd6a-438f-8f65-b6c84f6e25a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_df['Review'] = correlation_df['Review'].replace({'Prog1': 'Review 1', \n",
    "                                                             'Prog3': 'Review 2'})\n",
    "correlation_df['Characteristic'] = correlation_df['Characteristic'].replace({'percentage_reported': 'Percentage of TRIPOD criteria reported', \n",
    "                                                                             'word_count': 'Number of words in abstract',\n",
    "                                                                             'avg_sentence_length':'Average number of words per sentence',\n",
    "                                                                             'structured': 'Structured vs unstructured abstracts',\n",
    "                                                                             'tfidf_deviation': 'Abstract terminology usage (TF-IDF deviation)'})\n",
    "\n",
    "correlation_df['Ranking method'] = correlation_df['Ranking method'].replace({'ranking_average_logistic_tfidf': 'TF-IDF', \n",
    "                                                                             'ranking_average_logistic_sbert': 'sBERT',\n",
    "                                                                             'ranking_average_logistic_doc2vec': 'Doc2Vec'})\n",
    "correlation_df_final = correlation_df.drop(columns=['Beta'])\n",
    "\n",
    "correlation_df_final.to_excel('../output/results/coefs_table.xlsx', index=False)  \n",
    "\n",
    "#correlation_df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22749d9e-1057-47a8-9aef-8e07fcbb52e8",
   "metadata": {},
   "source": [
    "Create figures for in the results that represent the associations between abstract characteristics and ranking positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27924f17-cae1-4b64-a07f-051336de7aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_map = {'Prog1': 'Review 1', 'Prog3': 'Review 2'}\n",
    "review_dic_merged = {rename_map.get(k, k): v for k, v in review_dic_merged.items()}\n",
    "\n",
    "for key, value in review_dic_merged.items():\n",
    "    review_dic_merged[key] = review_dic_merged[key].rename(columns={'ranking_average_logistic_tfidf': 'TF-IDF', \n",
    "                                                                    'ranking_average_logistic_sbert': 'sBERT',\n",
    "                                                                    'ranking_average_logistic_doc2vec': 'Doc2Vec'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32531070-985f-439e-88a7-4e344ad8b595",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "rankings = ['TF-IDF', 'sBERT', 'Doc2Vec']\n",
    "dfs = ['Review 1', 'Review 2']\n",
    "\n",
    "for df_name in dfs:\n",
    "    plt.figure(figsize=(12, 6))  \n",
    "    df = review_dic_merged[df_name]\n",
    "    N = len(df)  \n",
    "\n",
    "    for ranking in rankings:\n",
    "        df_sort = df.sort_values(by=ranking, ascending=True)\n",
    "        labels = df_sort['label_ta_included']\n",
    "        labels_series = pd.Series(labels)\n",
    "        total_ones = labels_series.sum()\n",
    "        cumulative_sum = labels_series.cumsum()\n",
    "        proportion_ones = cumulative_sum / total_ones\n",
    "        \n",
    "        plt.plot(range(N), proportion_ones, label=ranking)\n",
    "\n",
    "    plt.plot([0, N], [0, 1], 'k--', label='Manual screening')  \n",
    "\n",
    "    plt.title(df_name)\n",
    "    plt.xlabel(\"Number of screened records\")\n",
    "    plt.ylabel(\"Proportion of relevant records found (sensitivity/recall)\")\n",
    "    plt.legend()\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ae72cd-6997-4dcd-8849-569ec0c7d6b5",
   "metadata": {},
   "source": [
    "**(a) Abstract TRIPOD adherence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e15eaa40-0501-4ae0-a872-4a32e17ec63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "dfs = ['Review 1', 'Review 2'] \n",
    "\n",
    "marker_styles = ['o', 's', '^'] \n",
    "\n",
    "for df_name in dfs: \n",
    "    df = review_dic_merged[df_name]\n",
    "    df_plot1 = df[df['label_ft_included'] == 1].copy()\n",
    "    \n",
    "    x_variables = ['percentage_reported']\n",
    "    y_variables = ['TF-IDF', 'sBERT', 'Doc2Vec']\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    colors = sns.color_palette(\"deep\", len(y_variables))\n",
    "    \n",
    "    legend_handles = []\n",
    "\n",
    "    for i, y_var in enumerate(y_variables):\n",
    "        x = df_plot1['percentage_reported']\n",
    "        y = df_plot1[y_var]\n",
    "\n",
    "        df_clean = df_plot1[[x_variables[0], y_var]].dropna()\n",
    "        spearman_corr, _ = spearmanr(df_clean[x_variables[0]], df_clean[y_var])\n",
    "\n",
    "        sns.regplot(\n",
    "            x=x, y=y, scatter=False,  \n",
    "            line_kws={\"linewidth\": 2, \"linestyle\": \"dotted\", \"color\": colors[i]}\n",
    "        )  \n",
    "\n",
    "        scatter = plt.scatter(x, y, color=colors[i], marker=marker_styles[i], s=15, alpha=0.8)  \n",
    "\n",
    "        legend_handles.append(plt.Line2D(\n",
    "            [0], [0], marker=marker_styles[i], color='w',\n",
    "            markerfacecolor=colors[i], markersize=6, \n",
    "            label=f\"{y_var} (œÅ = {spearman_corr:.3f})\"\n",
    "        ))\n",
    "\n",
    "    plt.ylabel(\"Average ranking position\")\n",
    "    plt.xlabel(\"Percentage of applicable TRIPOD criteria reported\")\n",
    "\n",
    "    plt.legend(handles=legend_handles, loc='upper left', frameon=True)\n",
    "\n",
    "    plt.title(f'{df_name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca123ca1-7e12-4233-b1f6-69223f370644",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "rankings = ['TF-IDF', 'sBERT', 'Doc2Vec']\n",
    "main_colors = ['#1f77b4', '#ff7f0e', '#2ca02c']     \n",
    "light_colors = ['#c6dbef', '#fdd0a2', '#c7e9c0']   \n",
    "\n",
    "for i, ranking in enumerate(rankings):\n",
    "    color_main = main_colors[i]\n",
    "    color_light = light_colors[i]\n",
    "    \n",
    "    for df_name in dfs:\n",
    "        df_plot2 = review_dic_merged[df_name].copy()\n",
    "        \n",
    "        selected_columns = [ranking] + list(df_plot2.filter(regex='^(1|2)'))\n",
    "        df_selected = df_plot2[selected_columns]\n",
    "        \n",
    "        df_melted = df_selected.melt(id_vars=ranking, var_name='item', value_name='value')\n",
    "        df_melted[ranking] = pd.to_numeric(df_melted[ranking])\n",
    "        \n",
    "        fill_palette = {0: color_light, 1: color_main}\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.boxplot(\n",
    "            x=\"item\", \n",
    "            y=ranking, \n",
    "            hue=\"value\", \n",
    "            data=df_melted,\n",
    "            palette=fill_palette,\n",
    "            flierprops={\"marker\": \"o\", \"markersize\": 3}\n",
    "        )\n",
    "        \n",
    "        if df_name == 'Review 1':\n",
    "            plt.ylim(0, 1399)\n",
    "        elif df_name == 'Review 2':\n",
    "            plt.ylim(0, 3600)\n",
    "        \n",
    "        plt.ylabel(\"Average ranking position\")\n",
    "        plt.xlabel(\"TRIPOD criterion\")\n",
    "        \n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        plt.legend(loc='upper left', frameon=True,\n",
    "                   handles=handles, labels=['Not reported', 'Reported'])\n",
    "        plt.title(f'{df_name} - {ranking}')\n",
    "        \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3829c1a1-13a1-4c03-8af4-08a7ea10180b",
   "metadata": {},
   "source": [
    "To check the content of the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1ae6ae5-acc2-440e-ba9b-8475e92d1961",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "sorted_prog3 = review_dic_merged['Review 2'].sort_values(by='sBERT', ascending=False)\n",
    "print('outlier 1: \\n\\n', sorted_prog3['abstract'][0])\n",
    "print('\\noutlier 2: \\n\\n', sorted_prog3['abstract'][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
